{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing all assests from base.py...\n",
      "Imported modules   : numpy, pandas, matplotlib.pyplot\n",
      "Imported functions : npy, axes_off, get_var_name, shapes, tqdm, plot_history, minmax, values\n"
     ]
    }
   ],
   "source": [
    "from train_utils import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from base import *\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias, activation=None):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(in_features, out_features, bias)\n",
    "        self.act = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.act is None:\n",
    "            return self.layer(x)\n",
    "        \n",
    "        return self.act(self.layer(x))\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Requires: enc_channels, dec_channels, bias, activations (nn.ReLU(), nn.GELU(), etc)\n",
    "        \"\"\"\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        enc_channels, dec_channels = kwargs['enc_channels'], kwargs['dec_channels']\n",
    "        bias = kwargs['bias']\n",
    "        activations = kwargs['activations']\n",
    "\n",
    "        if enc_channels[-1] != dec_channels[0]: \n",
    "            print(\"[WARN] First shape of dec_channels does not match the terminal channel in enc_channels, proceeding with additional layer...\")\n",
    "            dec_channels = (enc_channels[-1],)+dec_channels\n",
    "\n",
    "        self.enc = nn.Sequential()\n",
    "        for i in range(len(enc_channels)-1):\n",
    "            self.enc.add_module(f'enc_dense{i}', DenseBlock(enc_channels[i], enc_channels[i+1], bias=bias, activation=activations))\n",
    "\n",
    "        self.dec = nn.Sequential()\n",
    "        for i in range(len(dec_channels)-1):\n",
    "            self.dec.add_module(f'dec_dense{i}', DenseBlock(dec_channels[i], dec_channels[i+1], bias=bias, activation=activations))\n",
    "\n",
    "    def forward(self, x, return_embed=False):\n",
    "        x = x.float()\n",
    "        embed = self.enc(x)\n",
    "        out = self.dec(embed)\n",
    "\n",
    "        if return_embed: return embed, out\n",
    "        return out\n",
    "\n",
    "class GroupedModel(nn.Module):\n",
    "    def __init__(self, n_clusters, model_class, **kwargs):\n",
    "        super(GroupedModel, self).__init__()\n",
    "        self.n_clusters = n_clusters\n",
    "\n",
    "        self.AE = nn.ModuleList(model_class(**kwargs) for i in range(n_clusters))\n",
    "\n",
    "    def _return_embed(self, embed, out, flag):\n",
    "        return (embed, out) if flag else out\n",
    "\n",
    "    def forward_with_clust(self, x, centers, clust, return_embed=False):\n",
    "        \"\"\" \n",
    "        To be used in the batched phase, computes output for input x, all belonging to the same cluster \n",
    "        \"\"\"\n",
    "        embed, out = self.AE[clust](x - centers[clust], True)\n",
    "\n",
    "        return self._return_embed(embed, out, return_embed)\n",
    "\n",
    "    def forward_with_centers(self, x, centers, return_embed=False):\n",
    "        \"\"\" \n",
    "        To be used in warmup phase, computes output for input x for all clusters. \n",
    "        Output format (batch, n_clusters, <data shape>)\n",
    "        \"\"\"\n",
    "        embed, out = self.AE[0](x - centers[0].reshape(x.shape[1:]), True)\n",
    "        embed, out = embed.reshape((len(x),1)+embed.shape[1:]), out.reshape((len(x),1)+out.shape[1:])\n",
    "\n",
    "        for n in range(1, self.n_clusters):\n",
    "            e, o = self.AE[n](x - centers[n].reshape(x.shape[1:]), True)\n",
    "            embed = torch.cat((embed, e.reshape((len(x),1)+e.shape[1:])), dim=1)\n",
    "            out = torch.cat((out, o.reshape((len(x),1)+o.shape[1:])), dim=1)\n",
    "\n",
    "        return self._return_embed(embed, out, return_embed)\n",
    "    \n",
    "    def forward(self, *args):\n",
    "        raise NotImplementedError(\"Use one of `forward_with_centers` or `forward_with_clust`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: inputs of shape 5 (1d), AE follows 5->2->1->2->5 (1 = embed_dim)\n",
    "\n",
    "kwargs = {'enc_channels': (5,2,1),\n",
    "          'dec_channels': (1,2,5), \n",
    "          'bias': False, \n",
    "          'activations': nn.ReLU()}\n",
    "\n",
    "ae = Autoencoder(**kwargs)\n",
    "gae = GroupedModel(2, Autoencoder, **kwargs)\n",
    "\n",
    "dummy_input = torch.randn(64, 5)\n",
    "dummy_true = torch.randn(64,5) \n",
    "dummy_centers = torch.randn(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed  : torch.Size([64, 1])\n",
      "out    : torch.Size([64, 5])\n",
      "embed  : torch.Size([64, 2, 1])\n",
      "out    : torch.Size([64, 2, 5])\n",
      "embed  : torch.Size([64, 1])\n",
      "out    : torch.Size([64, 5])\n"
     ]
    }
   ],
   "source": [
    "embed, out = ae(dummy_input, True)\n",
    "shapes(embed, out)\n",
    "\n",
    "embed, out = gae.forward_with_centers(dummy_input, dummy_centers, True)\n",
    "shapes(embed, out)\n",
    "\n",
    "embed, out = gae.forward_with_clust(dummy_input, dummy_centers, 0, True)\n",
    "shapes(embed, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_npy(x): \n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.detach().cpu().numpy()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import kmeans_plusplus\n",
    "\n",
    "class TensorizedAutoencoder(nn.Module):\n",
    "    def __init__(self, grouped_model, Y_data, regularizer_coef=0.01):\n",
    "        super(TensorizedAutoencoder, self).__init__()\n",
    "        self.n_clusters = grouped_model.n_clusters\n",
    "        self.centers = None\n",
    "        \n",
    "        self.autoencoders = grouped_model\n",
    "        self.reg = regularizer_coef\n",
    "\n",
    "        self.random_state = None    # set manually if needed, call model._init_clusters(Y) again\n",
    "        self._init_clusters(to_npy(Y_data))\n",
    "    \n",
    "    def mse(self, x, y, dim=None):\n",
    "        return F.mse_loss(x, y, reduction='none').mean(dim=dim) if dim is not None else F.mse_loss(x, y)\n",
    "    \n",
    "    def _init_clusters(self, Y):\n",
    "        centers = kmeans_plusplus(Y.reshape(len(Y), -1), n_clusters=self.n_clusters, random_state=self.random_state)[0]\n",
    "        self.centers = torch.from_numpy(centers)\n",
    "        assert self.centers.shape == (self.n_clusters, Y.shape[1])\n",
    "    \n",
    "    def forward_with_clust(self, x, clust, return_embed=False):\n",
    "        return self.autoencoders.forward_with_clust(x, self.centers, clust, return_embed)\n",
    "    \n",
    "    def forward_with_centers(self, x, return_embed=False):\n",
    "        return self.autoencoders.forward_with_centers(x, self.centers, return_embed)\n",
    "    \n",
    "    def assign_centers_to_data(self, data, one_hot=False, centers=None):\n",
    "        centers = centers or self.centers\n",
    "        centers = centers.reshape(self.n_clusters, -1)\n",
    "\n",
    "        assignments = torch.tensor([])\n",
    "        for i in range(self.n_clusters):\n",
    "            d = torch.norm(data.reshape(len(data), -1) - self.centers[i], dim=1).reshape(-1,1)\n",
    "            assignments = torch.cat((assignments,d), dim=1)\n",
    "        if one_hot: return F.one_hot(assignments.argmin(dim=1), self.n_clusters).T\n",
    "        return assignments.argmin(dim=1)\n",
    "\n",
    "    def update_centers(self, Y, clust_assign):\n",
    "        assert clust_assign.shape == (self.n_clusters, len(Y)), \"check if clust_assign is in one-hot format\"\n",
    "        clust_assign.to(Y.device)\n",
    "        new_centers = clust_assign.float() @ Y.reshape(len(Y), -1).float()\n",
    "        new_norm = torch.sum(clust_assign, axis=1, dtype=torch.float).reshape(-1, 1) @ \\\n",
    "                        torch.ones(1, self.centers.shape[1], dtype=torch.float)\n",
    "        new_centers = new_centers / new_norm\n",
    "        return new_centers\n",
    "    \n",
    "    def compute_loss_warmup(self, x, y, clust_assign):\n",
    "        clusts = clust_assign.argmax(dim=0)\n",
    "        b = x.shape[0]\n",
    "\n",
    "        x_, y_ = x.clone(), y.clone()\n",
    "\n",
    "        embed, out = self.autoencoders.forward_with_centers(x_, self.centers, True)\n",
    "        # (batch, n_clusters, <data shape>)\n",
    "\n",
    "        new_assignment = torch.zeros((self.n_clusters, b), dtype=int)\n",
    "        loss = 0\n",
    "        for samp in range(b):\n",
    "            true, e, o = y_[samp][None,:].repeat_interleave(self.n_clusters, dim=0), embed[samp], out[samp]\n",
    "\n",
    "            e_, o_ = e.reshape(self.n_clusters, -1), o.reshape(self.n_clusters, -1)\n",
    "\n",
    "            loss_proxy = self.mse(o_, true, dim=1) + self.reg * (torch.norm(e_, dim=1) ** 2)\n",
    "            # shapes(loss_proxy)\n",
    "\n",
    "            new_center = loss_proxy.argmin(dim=0)\n",
    "            loss += self.mse(o_[clusts[samp]], true[0]) + self.reg * (torch.norm(e_) ** 2)\n",
    "\n",
    "            new_assignment[:,samp][new_center] = 1\n",
    "\n",
    "        return loss, new_assignment\n",
    "\n",
    "    def _collect_data(self, x, y, clust_assign):\n",
    "        clusts = clust_assign.argmax(dim=0)\n",
    "        return [(i,x[clusts==i],y[clusts==i]) for i in torch.unique(clusts)]\n",
    "\n",
    "    def compute_loss_batch(self, x, y, clust_assign):\n",
    "        collected = self._collect_data(x, y, clust_assign)\n",
    "        loss = 0\n",
    "        for c,x,y in collected:\n",
    "            x_ = x.clone() - self.centers[c]\n",
    "            y_ = y.clone() - self.centers[c]\n",
    "\n",
    "            embed, out = self.forward_with_clust(x_, c, return_embed=True)\n",
    "            loss += self.mse(out, y_) + self.reg * (torch.norm(embed) ** 2)\n",
    "        \n",
    "        return loss / len(collected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tae(tae, X, Y, epochs, lr, batch_size, warmup=0.3, verbose=True, grad_clip=5):\n",
    "    clust_assign = tae.assign_centers_to_data(Y, one_hot=True)\n",
    "    tae.centers = tae.update_centers(Y, clust_assign)\n",
    "    \n",
    "    optimizer = torch.optim.Adagrad(tae.parameters(), 0.1)\n",
    "    warmup_dataloader = GenericDataset(X, Y).get_dataloader(batch_size=1, shuffle=False)\n",
    "    \n",
    "    warmup_epochs = int(warmup*epochs)\n",
    "\n",
    "    if verbose: print(f\"PHASE 1: Warmup — {warmup_epochs}/{epochs}\")\n",
    "    pbar_warmup = ProgressBar(warmup_epochs, 75, verbose)\n",
    "    warmup_losses = []\n",
    "    for epoch in pbar_warmup.bar:\n",
    "        batch_losses = []\n",
    "        new_clust_assign = torch.tensor([])\n",
    "        for n, (x,y) in enumerate(warmup_dataloader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            loss, new_assignment = tae.compute_loss_warmup(x, y, clust_assign[:,n:n+1])\n",
    "            new_clust_assign = torch.cat((new_clust_assign, new_assignment), dim=1)\n",
    "\n",
    "            # if new_assignment.argmax() != clust_assign[:, sl_clust].argmax(): print(f\"updated {samp}\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(tae.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "        clust_assign = new_clust_assign.clone()\n",
    "        tae.centers = (tae.update_centers(Y, clust_assign) + epoch * tae.centers) / (epoch + 1)\n",
    "\n",
    "        warmup_losses.append(np.mean(batch_losses))\n",
    "        pbar_warmup.update(warmup_losses[-1])\n",
    "\n",
    "    \n",
    "    clust_wise_data = tae._collect_data(X, Y, clust_assign)\n",
    "    dataloaders = [(c, GenericDataset(x, y).get_dataloader(batch_size=batch_size, shuffle=True)) for (c,x,y) in clust_wise_data]    \n",
    "\n",
    "    batched_epochs = (epochs - warmup_epochs)\n",
    "    \n",
    "    if verbose: print(f\"PHASE 2: Batched — {batched_epochs}/{epochs}\")\n",
    "    clust_losses = []\n",
    "    for data in dataloaders:\n",
    "        ae = tae.autoencoders.AE[data[0]]\n",
    "        optimizer = torch.optim.Adam(ae.parameters(), lr)\n",
    "        clust_losses.append(train_ae(ae, data[1], optimizer, epochs=batched_epochs, verbose=verbose)[1])\n",
    "        \n",
    "    # pbar_batched = ProgressBar(batched_epochs, 75, verbose)\n",
    "    # losses = []\n",
    "    # for epoch in pbar_batched.bar:\n",
    "    #     batch_losses = []\n",
    "    #     for n, (x,y) in enumerate(dataloader):\n",
    "    #         idx = n * batch_size\n",
    "\n",
    "    #         x_, y_ = x.to(device), y.to(device)\n",
    "    #         loss = tae.compute_loss_batch(x_, y_, clust_assign[:,idx:idx+batch_size])\n",
    "\n",
    "    #         optimizer.zero_grad()\n",
    "    #         loss.backward()\n",
    "    #         # nn.utils.clip_grad_norm_(tae.parameters(), grad_clip)\n",
    "\n",
    "    #         optimizer.step()\n",
    "\n",
    "    #         batch_losses.append(loss.item())\n",
    "        \n",
    "    #     losses.append(np.mean(batch_losses))\n",
    "    #     pbar_batched.update(losses[-1])\n",
    "\n",
    "    return warmup_losses, clust_losses, clust_assign.argmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE 1: Warmup — 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10/10 |███████████████████████████████████| [00:01<00:00, loss: nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE 2: Batched — 40/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 40/40 |████████████████████████████| [00:00<00:00, loss: nan, es: 0]\n"
     ]
    }
   ],
   "source": [
    "# train_tae working test\n",
    "\n",
    "tae = TensorizedAutoencoder(gae, dummy_true)\n",
    "warmup, batched, clusts = train_tae(tae, dummy_input, dummy_true, 50, 5e-3, 8, warmup=0.2, verbose=1, grad_clip=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from data.data import parallel_line, orthogonal, triangle, lines_3D\n",
    "from sklearn.metrics.cluster import adjusted_rand_score as ari\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X, Y, X_noise, n_clusters = parallel_line(noise=0.1)\n",
    "randperm = torch.randperm(len(X))\n",
    "X, Y, X_noise = X[randperm].float(), Y[randperm].float(), X_noise[randperm].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'enc_channels': (5,2),\n",
    "          'dec_channels': (2,5), \n",
    "          'bias': False, \n",
    "          'activations': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 50/50 |█████████████████████████| [00:00<00:00, loss: 0.0060, es: 7]\n"
     ]
    }
   ],
   "source": [
    "ae = Autoencoder(**kwargs)\n",
    "optimizer = torch.optim.Adam(ae.parameters(), 5e-3)\n",
    "dataloader = GenericDataset(X, X).get_dataloader(batch_size=8, shuffle=False)\n",
    "\n",
    "ae, losses = train_ae(ae, dataloader, optimizer, 50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1882917549396861"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ari(KMeans(n_clusters).fit(to_npy(ae.enc(X.float()))).labels_,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE 1: Warmup — 20/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20 |████████████████████████████████| [00:04<00:00, loss: 0.7715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE 2: Batched — 30/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 30/30 |█████████████████████████| [00:00<00:00, loss: 0.0796, es: 0]\n",
      "Epoch: 30/30 |█████████████████████████| [00:00<00:00, loss: 0.0309, es: 0]\n"
     ]
    }
   ],
   "source": [
    "gae = GroupedModel(n_clusters, Autoencoder, **kwargs)\n",
    "tae = TensorizedAutoencoder(gae, X)\n",
    "\n",
    "warmup_losses, losses, clusts = train_tae(tae, X.float(), X.float(), 50, 5e-3, 8, 0.4, verbose=1, grad_clip=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733321641655686"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ari(clusts, Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with reconstruction mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 50/50 |█████████████████████████| [00:00<00:00, loss: 0.0061, es: 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE 1: Warmup — 20/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20 |████████████████████████████████| [00:05<00:00, loss: 1.0930]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE 2: Batched — 30/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 30/30 |█████████████████████████| [00:00<00:00, loss: 0.0236, es: 1]\n",
      "Epoch: 30/30 |█████████████████████████| [00:00<00:00, loss: 0.0255, es: 0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAE: 0.0457\n",
      "AE: 0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_runs = 1\n",
    "verbose = 1\n",
    "\n",
    "lae, ltae, lkm = [], [], []\n",
    "for run in tqdm(range(num_runs), disable=verbose):\n",
    "    # kmeans_ari = ari(KMeans(n_clusters).fit(to_npy(X)).labels_,Y)\n",
    "    \n",
    "    ae = Autoencoder(**kwargs)\n",
    "    optimizer = torch.optim.Adam(ae.parameters(), 5e-3)\n",
    "    dataloader = GenericDataset(X, X).get_dataloader(batch_size=8, shuffle=False)\n",
    "\n",
    "    ae, losses = train_ae(ae, dataloader, optimizer, 50, verbose=verbose)\n",
    "    # ae_ari = ari(KMeans(n_clusters).fit(to_npy(ae.enc(X.float()))).labels_,Y)\n",
    "\n",
    "    ae_mse = mse(to_npy(ae(X)), X)\n",
    "\n",
    "    gae = GroupedModel(n_clusters, Autoencoder, **kwargs)\n",
    "    tae = TensorizedAutoencoder(gae, X)\n",
    "\n",
    "    warmup_losses, losses, clusts = train_tae(tae, X.float(), X.float(), 50, 5e-3, 8, 0.4, verbose=verbose, grad_clip=0.5)\n",
    "    # tae_ari = ari(clusts, Y)\n",
    "\n",
    "    tae_mse = np.zeros(X.shape)\n",
    "    for i in range(len(X)):\n",
    "        tae_mse[i] = to_npy(tae.forward_with_clust(X[i:i+1], clusts[i:i+1]) + tae.centers[clusts[i]])\n",
    "\n",
    "    tae_mse = mse(tae_mse, X)\n",
    "\n",
    "    lae.append(ae_mse); ltae.append(tae_mse)\n",
    "\n",
    "print(f\"TAE: {np.mean(ltae):.4f}\")\n",
    "print(f\"AE: {np.mean(lae):.4f}\")\n",
    "\n",
    "# print(f\"Kmeans after TAE: {np.mean(ltae):.4f}\")\n",
    "# print(f\"Kmeans after AE: {np.mean(lae):.4f}\")\n",
    "# print(f\"Direct kmeans: {np.mean(lkm):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
